# -*- coding: utf-8 -*-
"""dl_pa2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zSatNR7am9tuewG-5t2Yh5uh4atFCTrV
"""


# Commented out IPython magic to ensure Python compatibility.
# %pdb on

import pathlib
import matplotlib.pyplot as plt
import numpy as np
import pickle
import random as r
import tensorflow as tf
import pandas as pd
from google.colab import drive
import math

drive.mount('/content/gdrive')

training_labels = np.loadtxt('/content/gdrive/My Drive/Colab_Notebooks/pa_2/train_label.txt', dtype = int) # Make sure folders and your python script are in the same directory. Otherwise, specify the full path name for each folder.
# convert 0-9 and build one hot vectors
training_labels = np.array( (tf.reshape(tf.one_hot(training_labels , 10, dtype=tf.float32), (-1, 10))))
testing_labels = np.loadtxt('/content/gdrive/My Drive/Colab_Notebooks/pa_2/test_label.txt', dtype = int) # Make sure folders and your python script are in the same directory. Otherwise, specify the full path name for each folder.
testing_labels = np.array((tf.reshape(tf.one_hot(testing_labels , 10, dtype=tf.float32), (-1, 10))))

#--Block-- Pickle block so we don't need to convert data all the time
train_set = np.array(pickle.load( open( "/content/gdrive/My Drive/Colab_Notebooks/pa_2/training_set_2.txt", "rb" ) ))
test_set = np.array(pickle.load( open( "/content/gdrive/My Drive/Colab_Notebooks/pa_2/testing_set_2.txt", "rb" ) ))
#--Block--

def update(Theta, w_grad, minibatch_size=50):
  
  for i in range(len(Theta)):
    Theta[i] = Theta[i] - np.multiply(np.multiply(w_grad[i],1/minibatch_size),lr)
  return Theta

def forward(Theta, X, num_hidden_layers):
  H = [0]*(num_hidden_layers+1)
  H[0] = X

  for i in range(0,num_hidden_layers):
    H[i+1] = np.concatenate((ReLU(H[i],Theta[2*i:2*i+2]),[1]),axis = None).reshape(1,3)
  y_hat = softmax(H[num_hidden_layers],Theta[2*num_hidden_layers:2*num_hidden_layers+2])
  return tf.cast(y_hat, tf.float32), H
